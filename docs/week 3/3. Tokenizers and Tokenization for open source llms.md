# Tokenizers and Tokenization in Open Source Models

## #Goals -
![img_18.png](img_18.png)

## Tokenizers
Tokenizers are essential components in natural language processing (NLP) that convert text into a format that machine learning models 
can understand. They break down text into smaller units, such as words or subwords, which are then mapped to numerical representations.

There are two maain functions of tokenizers:
1. encoding - This is the process of converting text into tokens.
2. decoding - This is the process of converting tokens back into text.

Tokenizers contains a vocabulary that maps each token to a unique integer ID. 
This is a collection of all of the fragment of characters that the tokenizer can recognize.
There can also be some special tokens that are used to indicate the start and end of a sequence, padding, etc.

**Note - How does the neural networks know a token is a special token or not that has to be used for starting sentence or padding etc?
Answer is there has to be enough data on which the neural network is trained to recognize these special tokens.**


3. Chat Template - Tokenizers in addition to doing the encoding and decoding, they also provide a chat template that is used to format the input text for the model.
   - This is used to format the input text in a way that the model can understand.
   - It is used to add special tokens like start of sequence, end of sequence, padding, etc.

![img_19.png](img_19.png)

### Note - Every model in HuggingFace has its own tokenizer. It depends on which tokenizers was used to train the model.We need to use exactly the same tokenizer during inference time or runtime otherwise we would not get good results.

### For this lesson , we will look at the tokenizers for below models:
![img_21.png](img_21.png)



Lets Also start with out lab at this point
https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing#scrollTo=C9zvDGWD5pKp

First we need to install the transformers library.
```bash
!pip install -q transformers
```

Then we import the necessary libraries.
```python
from google.colab import userdata
from huggingface_hub import login
from transformers import AutoTokenizer
```

Then we login into HuggingFace using the token that we have created.
```python
hf_token = userdata.get('HF_TOKEN')
login(hf_token, add_to_git_credential=True)
```

**Note - we will use llama model on HuggingFace for this lesson. When using llama on huggingFace we first need to sign the terms of use**


## Accessing Llama 3.1 from Meta
### Creating our Tokenizer.
For creating a tokenizer instance for any model below is the single line of code.

```python
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)
```
AutoTokenizer is a class that automatically selects the appropriate tokenizer for the model.
we invoke the method from_pretrained() to load the tokenizer for the model.

**Note- only after getting access after signing the terms of use, we can use the model name 'meta-llama/Meta-Llama-3.1-8B'.**

Now we can use the tokenizer to encode and decode text.
#### Encoding Text
```python
text = "I am excited to show Tokenizers in action to my LLM engineers"
tokens = tokenizer.encode(text)
tokens
```
This will give us a list of token IDs that represent the text.

Output -
[128000,
 40,
 1097,
 12304,
 311,
 1501,
 9857,
 12509,
 304,
 1957,
 311,
 856,
 445,
 11237,
 25175]


**Note - In general one token represents four characters. Since there are 61 characters in the text, we have 15 tokens.**
```python
len(text) # 61
len(tokens) # 15
```


### Decoding Tokens
```python
tokenizer.decode(tokens)
```
**Output-**
<|begin_of_text|>I am excited to show Tokenizers in action to my LLM engineers

In the output we see that the text is decoded back to its original form.
Also there is a special token that indicates the start of the sequence of text.
This is because in all of the training examples that the model has seen, the text always starts with this special token.
SO it got trained with tokens like this therefore to get accurate results we need to use the same token duing inference time as well.



#### If we want to see how each token is mapped to a word or subword, we can use the tokenizer's batch_decode method .
```python
tokenizer.batch_decode(tokens)
```
**Output -**
['<|begin_of_text|>',
 'I',
 ' am',
 ' excited',
 ' to',
 ' show',
 ' Token',
 'izers',
 ' in',
 ' action',
 ' to',
 ' my',
 ' L',
 'LM',
 ' engineers']

**Observations :** 
1. Here we can see excited is a single token and not the general rule that 4 characters make a token.
This is because the tokenizer has a vocabulary that maps each token to a unique word or subword. since exceited is a common word, 
it is mapped to a single token.

2. As with the GPT's tokenizer , the fact that something is a beggining of a word and space is there when the word begin , 
   therefore space is also part of the word. Therefore ' am' is a different token than 'am'.

3. Tokenizers are case sensitive.


### Tokenizer Vocab
We can also see the vocabulary of the tokenizer.
```python
tokenizer.vocab
```

Output
'Į': 234,
 'ĠDiabetes': 53689,
 'ĠosÃ³b': 94543,
 'mas': 7044,
 'floating': 83610,
 '729': 22194,
 'Ġhits': 13280,
 'Ä±nÄ±za': 117979,
 'Ø¯ÛĮ': 102567,
 'æŁ¥çľĭæĳĺè¦ģ': 122614,
 'Ġparadise': 50852,
 'ĠØ°Ùĩ': 125075,
 'ĠÐ¼Ð°ÑĪÐ¸Ð½': 125895,
 '=params': 58743,
 '(en': 46708,
 '<usize': 91344,
...
...
...


### Tokenizer Special Tokens
We can also see the special tokens of the tokenizer.
```python
tokenizer.special_tokens_map
```
Output-
{'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}

Also using the below code -

```python
tokenizer.get_added_vocab()
```
![img_22.png](img_22.png)





# Instruct variants of Models and apply_chat_template method
These are models specially trained for chats.
These are typically labelled with the word "Instruct" at the end.
**They have been trained to expect prompts with a particular format that includes system, user and assistant prompts. This is how these models have been trained on**

There is a utility method apply_chat_template that will convert from the messages list format we are familiar with, 
into the right input prompt for this model.


**Note - Here we will see why the format of the prompt is like this.**

1. Lets define our AutoTokenizer class to get an instance of tokenizer for the model we pass.
```python
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)
```

Here we are using the instruct version of the model we used previously.
2. We define a message in the format of GPT as we have seen before. Then we invoke the apply_chat_template() on this message
   and this method will convert the message into the right structure to be used for this particular model.

```python

messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Tell a light-hearted joke for a room of Data Scientists"}
  ]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
print(prompt)
```

We have passed the tokenize=False , otherwise if it is true the output will be in tokens so we would not be able to understand it.
Lets see how this message gets converted into a aprticular format for this model.

output - exactly as it is -
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 26 Jul 2024

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>


**When learning about calling the llama3.2 using the API we passed the same prompt syntax as fdefined by openai and it still worked.
This is the reason why. Because internally llama will convert the prompt to this format , tokenize it, then pass it to the model llama3.2**



### Comparing the tokens generated for various models.
We will try out with phi3 , llama and starcoder2

```python
PHI3_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
STARCODER2_MODEL_NAME = "bigcode/starcoder2-3b"

llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)
phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)

text = "I am excited to show Tokenizers in action to my LLM engineers"
print(llama_tokenizer.encode(text))
print()
tokens = phi3_tokenizer.encode(text)
print(tokens)
```

output -
[128000, 40, 1097, 12304, 311, 1501, 9857, 12509, 304, 1957, 311, 856, 445, 11237, 25175]

[306, 626, 24173, 304, 1510, 25159, 19427, 297, 3158, 304, 590, 365, 26369, 6012, 414]

As we see for each model the generated tokens are completely different.

To see how the tokens are mapped to words for phi3 , lets do batch decode.


```python

llama_token = llama_tokenizer.encode(text)
print(llama_token)
print(tokenizer.batch_decode(llama_token))

tokens = phi3_tokenizer.encode(text)
print(phi3_tokenizer.batch_decode(tokens))
```
Output -'
['<|begin_of_text|>', 'I', ' am', ' excited', ' to', ' show', ' Token', 'izers', ' in', ' action', ' to', ' my', ' L', 'LM', ' engineers']
['I', 'am', 'excited', 'to', 'show', 'Token', 'izers', 'in', 'action', 'to', 'my', 'L', 'LM', 'engine', 'ers']
'


### Lets also see now how the chat template is applied to llama and phi3
```python
messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Tell a light-hearted joke for a room of Data Scientists"}
  ]

print(llama_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
print()
print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
```

Output
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 26 Jul 2024

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>



<|system|>
You are a helpful assistant<|end|>
<|user|>
Tell a light-hearted joke for a room of Data Scientists<|end|>
<|assistant|>


As we can see the structure for llama and phi are completely different.
We already saw the structure for llama previosuly.

For phi3 , there is a special tag for system , special for user and special for assistant.



### Lets also see StartCoder2 which is a specialized model for Coding (By ServiceNow and HuggingFace)
```python
starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)
code = """
def hello_world(person):
  print("Hello", person)
"""
tokens = starcoder2_tokenizer.encode(code)
for token in tokens:
  print(f"{token}={starcoder2_tokenizer.decode(token)}")
```

Output-
222=

610=def
17966= hello
100=_
5879=world
45=(
6427=person
731=):
353=
 
1489= print
459=("
8302=Hello
411=",
4944= person
46=)
222=

