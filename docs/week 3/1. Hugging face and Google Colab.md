# HuggingFace
HuggingFace is the leading open source platform for Natural Language Processing (NLP) and Machine Learning (ML). 
It provides a wide range of tools, libraries, and models that make it easier to work with NLP tasks.

Below are some of the key features of HuggingFace:
![img.png](img.png)

## huggingFace spaces is a platform that allows you to create and share machine learning models and applications and then expose those apps
## on huggingface cloud for other people to use.

**Note - Many of the apps on HuggingFace are built using the Gradio library. There is another popular library - streamlit**
 
## HuggingFace also has a bunch of libraries that make it easier to work with NLP tasks.
1. One of the most popular library is the HuggingFace Hub.
This allows us to login into HuggingFace and download and upload models and datasets.

2. datasets - This is a library that gives us access to a wide range of datasets for NLP tasks.

3. transformers - This is a library that provides a wide range of pre-trained models for NLP tasks.
                This is a central library which is the wrapper code around LLMs that follow the transformer architecture.

4. peft - stands for Parameter-Efficient Fine-Tuning. This is a utility that allows us to fine-tune pre-trained models with fewer parameters.

5. trl - stands for Transformer Reinforcement Learning. This is a library that allows us to use reinforcement learning with transformer models.
        It includes things like -
            1. Ability to do **reward modeling(RM)**
            2. **Proximal Policy Optimization (PPO)** for training models with RL
            3. **Supervised Fine-Tuning (SFT)** for training models with supervised learning

   6. accelerate - This is a library that allows us to train models on multiple GPUs and TPUs. It's some advanced huggingFace code that
                   allows us to run across any distributed configuration. Both for training and inference.
   



### When we sign in into HuggingFace , we have the model tab which shows us the models that we have uploaded or downloaded. This is Hub
![img_1.png](img_1.png)



# Google Colab
Google Colab is a free cloud-based Jupyter notebook environment that allows you to run Python code in the cloud.
The code runs on Google's servers, which would have good cpu and gpu.
![img_2.png](img_2.png)

## Colab is a great way to run code without having to set up a local environment.
We see the below screen when we open a new notebook.
In the Connect button like we see below-
![img_3.png](img_3.png)

We can select the hardware accelerator that we want to use.These are basically the GPU or CPU where we want to run our code.

Select Change Runtime type and then select the hardware accelerator.
![img_4.png](img_4.png)

CPU is in the free tier and does not have a GPU required to run the parallel matrix multiplications that are generally for neural networks.
Then we have the T4 GPU which is a good GPU for training and inference.
Then we have the L4 GPU which is a more powerful GPU for training and inference.
Finally, we have the strongest one that is A100.


Then we connect to the server .
We can see the resources of the server we connected to by clicking on the resources.
![img_5.png](img_5.png)

## Now we can start writting and running code in the notebook.
![img_7.png](img_7.png)
Since we selected CPU there is no GPU shown here.
If there we would have selected something like T4 then we will also see the GPU here.

### There are a couple of options on the left hand side of the screen.
1. The key icon is for storing our secrets like API keys, passwords, etc. 
   This is a good way to store secrets without hardcoding them in the code.
    Here we put the environment variables that we need to use in our code.

