# HuggingFace
HuggingFace is the leading open source platform for Natural Language Processing (NLP) and Machine Learning (ML). 
It provides a wide range of tools, libraries, and models that make it easier to work with NLP tasks.

Below are some of the key features of HuggingFace:
![img.png](img.png)

## huggingFace spaces is a platform that allows you to create and share machine learning models and applications and then expose those apps
## on huggingface cloud for other people to use.

**Note - Many of the apps on HuggingFace are built using the Gradio library. There is another popular library - streamlit**
 
## HuggingFace also has a bunch of libraries that make it easier to work with NLP tasks.
1. One of the most popular library is the HuggingFace Hub.
This allows us to login into HuggingFace and download and upload models and datasets.

2. datasets - This is a library that gives us access to a wide range of datasets for NLP tasks.

3. transformers - This is a library that provides a wide range of pre-trained models for NLP tasks.
                This is a central library which is the wrapper code around LLMs that follow the transformer architecture.

4. peft - stands for Parameter-Efficient Fine-Tuning. This is a utility that allows us to fine-tune pre-trained models with fewer parameters.

5. trl - stands for Transformer Reinforcement Learning. This is a library that allows us to use reinforcement learning with transformer models.
        It includes things like -
            1. Ability to do **reward modeling(RM)**
            2. **Proximal Policy Optimization (PPO)** for training models with RL
            3. **Supervised Fine-Tuning (SFT)** for training models with supervised learning

   6. accelerate - This is a library that allows us to train models on multiple GPUs and TPUs. It's some advanced huggingFace code that
                   allows us to run across any distributed configuration. Both for training and inference.
   



### When we sign in into HuggingFace , we have the model tab which shows us the models that we have uploaded or downloaded. This is Hub
![img_1.png](img_1.png)



# Google Colab
Google Colab is a free cloud-based Jupyter notebook environment that allows you to run Python code in the cloud.
The code runs on Google's servers, which would have good cpu and gpu.
![img_2.png](img_2.png)

## Colab is a great way to run code without having to set up a local environment.
We see the below screen when we open a new notebook.
In the Connect button like we see below-
![img_3.png](img_3.png)

We can select the hardware accelerator that we want to use.These are basically the GPU or CPU where we want to run our code.

Select Change Runtime type and then select the hardware accelerator.
![img_4.png](img_4.png)

CPU is in the free tier and does not have a GPU required to run the parallel matrix multiplications that are generally for neural networks.
Then we have the T4 GPU which is a good GPU for training and inference.
Then we have the L4 GPU which is a more powerful GPU for training and inference.
Finally, we have the strongest one that is A100.


Then we connect to the server .
We can see the resources of the server we connected to by clicking on the resources.
![img_5.png](img_5.png)

## Now we can start writting and running code in the notebook.
![img_7.png](img_7.png)
Since we selected CPU there is no GPU shown here.
If there we would have selected something like T4 then we will also see the GPU here.

### There are a couple of options on the left hand side of the screen.
1. The key icon is for storing our secrets like API keys, passwords, etc. 
   This is a good way to store secrets without hardcoding them in the code.
    Here we put the environment variables that we need to use in our code.



### Note - What is the difference between Ollama and HuggingFace?
**With HuggingFace, you're actually running the Deep Neural Network itself, using PyTorch code. 
So you have access to all the weights and structure. You can work with any model on HuggingFace, or create your own variant. 
You have control over the tokens that are passed in, over how the outputs are generated, and see the probability of each of the next tokens.
You can fine-tune your own specialized version of the model.
This is the kind of activity that an LLM Engineer might do.**

**With Ollama, someone has taken the Deep Neural Network and converted it to high performance C++ code. 
For specific models, a special file of the weights has been created. You can run this locally.**

**So the differences are:**

**- Ollama (llama.cpp) is for inference only**

**- With Ollama, you have limited control / flexibility on what happens - you're running the model out of the box**

**- With Ollama, you can only use the specific models that have been prepared for high performance inference**

This is an over-simplifying analogy, but you could think of Ollama as treating LLMs like a packaged Application, 
and HuggingFace as working at the code level. Hope that makes sense!**


