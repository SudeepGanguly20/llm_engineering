## Goal For Today-
![img_8.png](img_8.png)

### Lets start with HuggingFace APIs.
There are two different kinds of APIs available in HuggingFace -
1. **Pipelines** - 
    1. These are high level APIs that are easy to use and can be used for most of the tasks. 
    2. These are used to carry out standard tasks like inference , text classification, text generation, etc.

3. **Tokenizers and Models** - 
     1. This is used when we want to go deeper into the code like tokenizing texts , looking at parameters to run a model or fine tuning a model.
   

## Pipelines API
First we look at the pipelines API.
1. Can be used to take advantage of the pre-trained models available in HuggingFace using two line codes.
2. Use cases include -
   1. sentiment analysis
   2. classification
   3. Named Entity Recognition (NER) - This is when we want to identify entities like names, places, etc. in a text sentence.
   4. Question answering - This is when there is some context and we want to ask questions about the context.
   5. Summarization - This is when we want to summarize a long text into a shorter text.
   6. Translation - This is when we want to translate a text from one language to another.


3. Using Pipelines we can not only generate text but also images and audio.

**Step 1 : We need to install the transformers library along with the diffusers library and datasets for getting the data.**
```bash# Disregard the error that pip gives you when you run this - all should be well!
!pip install -q diffusers transformers accelerate bitsandbytes datasets fsspec==2023.9.2
```
**Step 2 : We need to set our HuggingFace api key in the secret section of the colab.**
![img_9.png](img_9.png)

**Step 3 : We can now use create our pipeline by 2 lines of code.**
```
my_pipeline = pipeline("the_task_I_want_to_do")
```
### The pipeline API will automatically download the model and tokenizer for the task we want to do.
The HuggingFace pipeline API syntax requires you to specify the task type as the first argument (a string keyword). This tells the pipeline which kind of model and processing to use.

Like in the example below we see we pass something like 
sentiment-analysis, ner, question-answering, summarization, translation, etc. as the first argument to the pipeline function. etc

```python

Lets see some examples of how to use the pipelines API.
https://colab.research.google.com/drive/1aMaEw8A56xs0bRM4lu8z7ou18jqyybGm?usp=sharing

**1. Sentiment Analysis Task**
```python
# Sentiment Analysis

classifier = pipeline("sentiment-analysis", device="cuda")
result = classifier("I'm super excited to be on the way to LLM mastery!")
print(result)
```
When we defin he pipeline API , we pass the name of the pipeline we want to give and 
The second parameter is the device we want to use. If we do not tell the notebook to use the GPU already available in the colab, it will use the CPU by default.

We can also pass the model that we want to use. If we do not pass the model, it will use the default model for that pipeline.
Our output-
![img_10.png](img_10.png)

In the response , the model gives us a label and a score.

If we change the statement and then run the same code again, we will get a different output.
![img_11.png](img_11.png)


**Note - When we run the code for first time , the model is downloaded from HuggingFace and cached in the local directory.
When we run the code again, it will use the cached model and not download it again.**


**2. Named Entity Recognition**
Named Entity Recognition (NER) is a task where we want to identify entities like names, places, etc. in a text sentence.
```python
ner = pipeline("ner", grouped_entities=True, device="cuda")
result = ner("Barack Obama was the 44th president of the United States.")
print(result)
```
Output -
[
{'entity_group': 'PER', 'score': np.float32(0.99918306), 'word': 'Barack Obama', 'start': 0, 'end': 12}
{'entity_group': 'LOC', 'score': np.float32(0.9986908), 'word': 'United States', 'start': 43, 'end': 56}
]

Here the output returns us a list of dictionaries where each dictionary contains the entity group, score, word, start and end position of the entity in the text.
first is PER for person and second is LOC for location.


**3. Question Answering with Context**
Here we can ask a question about a context and the model will return the answer.

```python
# Question Answering with Context
question_answerer = pipeline("question-answering", device="cuda")
result = question_answerer(question="Who was the 44th president of the United States?", context="Barack Obama was the 44th president of the United States.")
print(result)
```
Output -
{'score': 0.9889456033706665, 'start': 0, 'end': 12, 'answer': 'Barack Obama'}

**Note - As we can see there is a pattern to the response here to any type of use case of pipeline api. Each response has a answer and a score. 
The score is the confidence of the model in the answer it has given.**



**4. Text Summarization**
```python
# Text Summarization

summarizer = pipeline("summarization", device="cuda")
text = """The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).
It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.
It's an extremely popular library that's widely used by the open-source data science community.
It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.
"""
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
print(summary[0]['summary_text'])
```
 we pass tbe text that we want to summarize and the maximum and minimum length of the summary that we want.
Output -
The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing . 
 It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering .


**5. Translation**
This model is used to translate text from one language to another.
```python
# Translation

translator = pipeline("translation_en_to_fr", device="cuda")
result = translator("The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.")
print(result[0]['translation_text'])
```
Output -
![img_13.png](img_13.png)

### Lets also try to do the same by speciffying the model we want to use.
```python
# Another translation, showing a model being specified
# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending

translator = pipeline("translation_en_to_es", model="Helsinki-NLP/opus-mt-en-es", device="cuda")
result = translator("The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.")
print(result[0]['translation_text'])
```
![img_14.png](img_14.png)


**6. Classification**
Classification is a task where we want to classify a text into one of the predefined categories.
Same as Labelling.

```python
# Classification

classifier = pipeline("zero-shot-classification", device="cuda")
result = classifier("Hugging Face's Transformers library is amazing!", candidate_labels=["technology", "sports", "politics"])
print(result)
```
output -
{'sequence': "Hugging Face's Transformers library is amazing!", 
'labels': ['technology', 'sports', 'politics'], 
'scores': [0.9493839740753174, 0.03225007280707359, 0.018365919589996338]
}


**7. Text Generation**
Where we generate text based on a given prompt. This is similar to how we use chatbots.
```python
# Text Generation

generator = pipeline("text-generation", device="cuda")
result = generator("If there's one thing I want you to remember about using HuggingFace pipelines, it's")
print(result[0]['generated_text'])
```
Output -
![img_15.png](img_15.png)


## Note - For text related tasks we were using the transformers library until now.For image generation we need to install the diffusers library.


**8. Image Generation**
```python
# Image Generation

image_gen = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
    ).to("cuda")

text = "A class of Data Scientists learning about AI, in the surreal style of Salvador Dali"
image = image_gen(prompt=text).images[0]
image
```
Output -
![img_16.png](img_16.png)



**9. Audio Generation**
```python
# Audio Generation

synthesiser = pipeline("text-to-speech", "microsoft/speecht5_tts", device='cuda')

embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
speaker_embedding = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)

speech = synthesiser("Hi to an artificial intelligence engineer, on the way to mastery!", forward_params={"speaker_embeddings": speaker_embedding})

sf.write("speech.wav", speech["audio"], samplerate=speech["sampling_rate"])
Audio("speech.wav")
```
Output-
![img_17.png](img_17.png)

**More Information -**
https://huggingface.co/docs/transformers/main_classes/pipelines
https://huggingface.co/docs/diffusers/en/api/pipelines/overview