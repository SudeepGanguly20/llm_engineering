# The rise of Transformers Architecture

## Timeline
![img_1.png](images/img_16.png)

# How the world saw the rise of these technologies and why call them "emergent intelligence"
![img_2.png](images/img_17.png)
Where we are at this point is emergent Intelligence -
So while it is true that the outcomes are statistical predictions. We are provising the models with lots of data to learn the 
patterns and then ask them given all the patterns that you have seen in the training data what would be the most likely next token , 
what is the most likely nexr token after that.(Basically which of the token has the highest probability of being the next token).

With massive massive scale of trillions of weights and billions of parameters, these models are able to learn the patterns in the data and 
generate text that is coherent and relevant to the input. This is why we call these models "emergent intelligence" - 
they are able to generate text that is coherent and relevant to the input, even though they are not explicitly programmed to do so.


# The Journey So Far
![img_3.png](images/img_18.png)

# Lets see some more on the Models


## 1. Number of Parameters (log scale)
1. Weights / Parameters are the values that the model learns during training.
2. These are kinds of the levers that controls the kind of output generated by the model when we give it an input.
3. Weights are set during the training phase of the model, and they are used to make predictions during inference.
   1. During Training the models see lots and lots of example and based on the examples it sees it shifts around the weights until 
        it gets better in predicting the next token.

4. In the past when regression models were used, the number of weights were in the range of hundreds , thousands or sometimes millions.
5. But with the advent of deep learning and neural networks, the number of weights has exploded to billions and trillions.
![img_4.png](images/img_19.png)
![img_5.png](images/img_20.png)



## 2. Introducing Tokens
1. Tokens are the basic units of text that the model uses to generate text.


![img_6.png](images/img_21.png)

3. In early days , tokens were single characters. But that required a lot of memory and computation.

4. Next models were trained on individual words as tokens.
   1. A vocab would be built that is an index of all possible words
   2. The next token could be anyone of the words in the vocab.
   3. Issue with this approach was that the vocab size was very large and it required a lot of memory to store the vocab.
   4. There also had to be special tokens for unknown words, punctuation, etc.

5. The next breakthrough was to use chunks of text as tokens.
   1. These chunks of texts may or maynot be complete words.
   2. Benefits of this approach-
      3. We can handle entities like names, places, etc. better and consider them as individual tokens.
      4. This also meant models got good at handling word stems. This is there is one beginning of a word but multiple potential endings.
   
Note - GPT provides a tokenizer that can be used to tokenize text into tokens. (https://platform.openai.com/tokenizer). Lets see some examples

### Example 1
![img_7.png](images/img_22.png)

As we can see above each of the individual colors is a token and there are total 16 tokens.

OBSERVATION 1 : ![img_8.png](images/img_23.png) like learning , the , innner
OBSERVATION 2 : see for is tokenized as " for" that is the break between words is also meaningful when tokenizing.


### Example 2 : 
![img_9.png](images/img_9.png)

Here masterers is a made up word and it is tokenized as "master" and "ers". 
This is because the tokenizer is trying to break the word into smaller chunks that are more meaningful.
This is what we meant by word stems. 
    The tokenizer is able to recognize that "master" is a meaningful word and "ers" is a suffix that can be added to it.

Handcrafter is another word that is not present in the vocab of the model and it is tokenized as "hand" and "crafter".
Therefore we are still able to get the meaning of the word even though it is not present in the vocab.


### Example 3 : Numbers
![img_10.png](images/img_10.png)

Here we see that the number 123456789 is tokenized as "123 456 789". That is 3 numbers sequences.
This is actually a property of GPT tokenizer but this may not happen with other tokenizers.

### As a Rule of Thumb in typical english writing , one token is approximately 4 characters.
### So a 1000 tokens maps to 750 words.



## Context Window
This is the next important property of LLMs.

![img_11.png](images/img_11.png)

1. The context window is the maximum number of tokens that the model can consider when generating the next token.
2. In our Day 1 application of summarizing a webpage, we were using the context window of 4096 tokens.
   1. The input token consisted of the system prompt and the user prompt.
   2. The user prompt consisted of the prompt + entire website in text form without the markup.
   3. Based on the input tokens , the output is generated and the output token count is also considered in the total token count.

**When disucssing with something like chatgpt , we may feel like the model is able to remember the entire conversation. 
It is able to continue the conversation and remember what was said earlier. But this is an illusion.
Here is what happens - Everytime we talk to the model, we send the entire conversation history as input (userprompts, inputs , responses).
This is the context window. We ask one question , chatgpt responds . We ask a followup question and chatgpt responds.
Then when we ask the next question , all the questions and responses from previous two questions are sent as input to the model.
and this is the context window of the model**

### Therefore the context window is the total of all the conversation untill this point ,the inputs and subsequent conversations up until the next token it is predicting.

**Therefore if we want to ask a LLM anything that can be from any of the work of shakespeare, we would need to have in the context window the 
1.2 million tokens that will cover the entire lifetime of work of shakespear.**



## API Cost
![img_12.png](images/img_12.png)

Vellum is a ai company which does many of the benchmarking for LLMs.
It has a leaderboard page where we can see the leaders based on various parameters.like reasoning, coding, etc. (https://www.vellum.ai/llm-leaderboard)
For affordability this is the leaderboard that we are interested in.
![img_13.png](images/img_13.png)

We can also comapre various models based on their cost per 1 million tokens.
Below is a chart for the cost and context window of various models.
![img_14.png](images/img_14.png)

As we can see the context window of Gemini Flash 2.5 is 1 million tokens. 
This means we can pass almost the entire lifetime work of shakespear(1.2 million) to a model in one prompt.

Also as we can see the price is actally decent. For a million token again amost entire lifetime work of shakespear, it will cost us 0.15$.

